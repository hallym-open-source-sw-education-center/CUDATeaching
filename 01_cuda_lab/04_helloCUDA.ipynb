{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_helloCUDA.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonggunlee/CUDATeaching/blob/master/01_cuda_lab/04_helloCUDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJsm8qxkByZK",
        "colab_type": "text"
      },
      "source": [
        "## CUDA Basic Programming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mFiaU4S9FJ2",
        "colab_type": "text"
      },
      "source": [
        "##GPU 코딩의 기본 단계 익히기\n",
        "\n",
        "- CPU 메모리 설정\n",
        "- CPU 메모리 데이터 설정\n",
        "- GPU 메모리 설정 : cudaMalloc(...)\n",
        "- CPU --> GPU 데이터 전송: cudaMemcpy(to, from, sizeofdata, cudaMemcpyHostToDevice)\n",
        "- GPU 함수 (Kernel) 수행\n",
        "- GPU --> CPU 연산 결과 데이터 전송: : cudaMemcpy(to, from, sizeofdata, cudaMemcpyDeviceToHost);\n",
        "- 연산 결과를 CPU에서 사용\n",
        "\n",
        "*  *  *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_UOYChfF_jf",
        "colab_type": "text"
      },
      "source": [
        "##n 값을 변경시켜가며 nvprof을 통해서 수행시간 및 데이터 전달 시간을 살펴보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8oWrLbL9mut",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fa6dcd52-7496-4b54-fe04-9b38c6f3d915"
      },
      "source": [
        "%%writefile cudabasic.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda.h>\n",
        "using namespace std;\n",
        "\n",
        "int *host_A, *host_C1, *host_C2;       // host data\n",
        "int *device_A, *device_C;   // results\n",
        "\n",
        "__global__ void vecAddOne(int *A, int *C, int N)\n",
        "{\n",
        "   int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    \n",
        "   if( i < N )\n",
        "      C[i] = A[i] + 1; \n",
        "}\n",
        "\n",
        "void vecAddOne_h(int *A1, int *C1, int N)\n",
        "{\n",
        "   for(int i=0;i<N;i++)\n",
        "      C1[i] = A1[i] + 1;\n",
        "}\n",
        "\n",
        "int main(int argc,char **argv)\n",
        "{\n",
        "   int n=1024*1024;\n",
        "   int nBytes = n*sizeof(int);\n",
        "   int block_size = 32, block_no = n / block_size; \n",
        "\n",
        "   // ===============================================================\n",
        "   // CPU 메모리 설정 \n",
        "   //\n",
        "   host_A = (int *)malloc(nBytes);\n",
        "   host_C1 = (int *)malloc(nBytes);    \n",
        "   host_C2 = (int *)malloc(nBytes);    \n",
        "\n",
        "   // ===============================================================    \n",
        "   printf(\"Allocating device memory on host..\\n\");\n",
        "   cudaMalloc((void **)&device_A, n*sizeof(int));\n",
        "   cudaMalloc((void **)&device_C, n*sizeof(int));\n",
        "   // ===============================================================    \n",
        "   printf(\"Copying to device..\\n\");\n",
        "   cudaMemcpy(device_A, host_A, n*sizeof(int),cudaMemcpyHostToDevice);\n",
        "   // ===============================================================\n",
        "   printf(\"Doing GPU Vector + 1 \\n\");\n",
        "   vecAddOne<<<block_no,block_size>>>(device_A, device_C, n);   \n",
        "   cudaDeviceSynchronize();\n",
        "   // ===============================================================\n",
        "   printf(\"Doing a CPU Vector add\\n\");    \n",
        "   vecAddOne_h(host_A, host_C1, n);\n",
        "   \n",
        "   cudaMemcpy(host_C2, device_C, n*sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "   // 결과 비교\n",
        "   printf(\"결과 비교\\n\");\n",
        "   for(int i=0; i<n;i++)\n",
        "   {\n",
        "       if(host_C1[i] != host_C2[i])\n",
        "       {\n",
        "           printf(\"Something Wrong ! \\n\");\n",
        "           break;\n",
        "       }\n",
        "   }\n",
        "   cudaFree(device_A);\n",
        "   cudaFree(device_C);\n",
        "   free(host_A);\n",
        "   free(host_C1);\n",
        "   free(host_C2);\n",
        "   return 0;\n",
        "}  "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting cudabasic.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulc4tfsuBHsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc -o cudabasic cudabasic.cu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp12avb1BcWe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "652543c6-8125-43a9-b920-1ecdefbf89f9"
      },
      "source": [
        "!./cudabasic"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Allocating device memory on host..\n",
            "Copying to device..\n",
            "Doing GPU Vector + 1 \n",
            "Doing a CPU Vector add\n",
            "결과 비교\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwC2L5UUGMtH",
        "colab_type": "text"
      },
      "source": [
        "*  *  *\n",
        "*  *  *\n",
        "## nvprof:\n",
        "\n",
        "\n",
        "참조:\n",
        "\n",
        "- https://devblogs.nvidia.com/cuda-pro-tip-nvprof-your-handy-universal-gpu-profiler/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vMCjO8ZFMMv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "17bb0333-053a-4180-faf0-eba1dd43ec59"
      },
      "source": [
        "!nvprof ./cudabasic"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Allocating device memory on host..\n",
            "==835== NVPROF is profiling process 835, command: ./cudabasic\n",
            "Copying to device..\n",
            "Doing GPU Vector + 1 \n",
            "Doing a CPU Vector add\n",
            "결과 비교\n",
            "==835== Profiling application: ./cudabasic\n",
            "==835== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   55.53%  1.6226ms         1  1.6226ms  1.6226ms  1.6226ms  [CUDA memcpy DtoH]\n",
            "                   40.72%  1.1897ms         1  1.1897ms  1.1897ms  1.1897ms  [CUDA memcpy HtoD]\n",
            "                    3.75%  109.72us         1  109.72us  109.72us  109.72us  vecAddOne(int*, int*, int)\n",
            "      API calls:   97.66%  228.51ms         2  114.26ms  294.69us  228.22ms  cudaMalloc\n",
            "                    1.94%  4.5426ms         2  2.2713ms  1.5016ms  3.0410ms  cudaMemcpy\n",
            "                    0.16%  363.22us         2  181.61us  145.53us  217.69us  cudaFree\n",
            "                    0.09%  203.17us         1  203.17us  203.17us  203.17us  cuDeviceTotalMem\n",
            "                    0.07%  153.12us         1  153.12us  153.12us  153.12us  cudaDeviceSynchronize\n",
            "                    0.06%  149.82us        96  1.5600us     130ns  64.457us  cuDeviceGetAttribute\n",
            "                    0.02%  39.996us         1  39.996us  39.996us  39.996us  cudaLaunchKernel\n",
            "                    0.01%  26.604us         1  26.604us  26.604us  26.604us  cuDeviceGetName\n",
            "                    0.00%  2.9230us         1  2.9230us  2.9230us  2.9230us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.3790us         3     793ns     166ns  1.7490us  cuDeviceGetCount\n",
            "                    0.00%  1.2660us         2     633ns     285ns     981ns  cuDeviceGet\n",
            "                    0.00%     246ns         1     246ns     246ns     246ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR4DTteiDXKg",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "## Thread와 Block 이해하기\n",
        "\n",
        "- blockIdx.x / blockIdx.y\n",
        "- blockDim.x / blockDim.y\n",
        "- threadIdx.x / threadIdx.y\n",
        "- threadDim.x / threadDim.y\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGRYhJOWBs8p",
        "colab_type": "code",
        "outputId": "0eb3bef5-63b7-4437-c6d8-ca4d4001e37f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile helloCUDA.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void helloCUDA(void)\n",
        "{\n",
        "  printf(\"Hello thread %d in block %d\\n\", threadIdx.x, blockIdx.x);\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  int n = 12;\n",
        "  int blockDim = 4;            // Block내의 Thread의 수\n",
        "  int gridDim = n / blockDim;  // Grid에서 Block의 수\n",
        "  \n",
        "  // 따라서, 전체 생성 thread의 수는 blockDim * threadDim  \n",
        "    \n",
        "  helloCUDA<<<gridDim, blockDim>>>();\n",
        "    \n",
        "  cudaDeviceSynchronize();\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting helloCUDA.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftSMqLF6B6gB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc -o helloCUDA helloCUDA.cu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwaUXUeECDi5",
        "colab_type": "code",
        "outputId": "15de0a3c-37c2-4dcf-ca22-815888fb2b2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "!./helloCUDA"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello thread 0 in block 2\n",
            "Hello thread 1 in block 2\n",
            "Hello thread 2 in block 2\n",
            "Hello thread 3 in block 2\n",
            "Hello thread 0 in block 1\n",
            "Hello thread 1 in block 1\n",
            "Hello thread 2 in block 1\n",
            "Hello thread 3 in block 1\n",
            "Hello thread 0 in block 0\n",
            "Hello thread 1 in block 0\n",
            "Hello thread 2 in block 0\n",
            "Hello thread 3 in block 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP-QEuUxGeZv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "a46e0612-db70-43bf-9dc3-cdf1e0ffcff6"
      },
      "source": [
        "!nvprof --print-gpu-trace ./helloCUDA"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==882== NVPROF is profiling process 882, command: ./helloCUDA\n",
            "Hello thread 0 in block 2\n",
            "Hello thread 1 in block 2\n",
            "Hello thread 2 in block 2\n",
            "Hello thread 3 in block 2\n",
            "Hello thread 0 in block 1\n",
            "Hello thread 1 in block 1\n",
            "Hello thread 2 in block 1\n",
            "Hello thread 3 in block 1\n",
            "Hello thread 0 in block 0\n",
            "Hello thread 1 in block 0\n",
            "Hello thread 2 in block 0\n",
            "Hello thread 3 in block 0\n",
            "==882== Profiling application: ./helloCUDA\n",
            "==882== Profiling result:\n",
            "   Start  Duration            Grid Size      Block Size     Regs*    SSMem*    DSMem*           Device   Context    Stream  Name\n",
            "411.15ms  92.760us              (3 1 1)         (4 1 1)        32        0B        0B     Tesla T4 (0)         1         7  helloCUDA(void) [106]\n",
            "\n",
            "Regs: Number of registers used per CUDA thread. This number includes registers used internally by the CUDA driver and/or tools and can be more than what the compiler shows.\n",
            "SSMem: Static shared memory allocated per CUDA block.\n",
            "DSMem: Dynamic shared memory allocated per CUDA block.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtwajQGsGjTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}