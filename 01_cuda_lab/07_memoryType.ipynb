{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_memoryType.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonggunlee/CUDATeaching/blob/master/01_cuda_lab/07_memoryType.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB9CAgYq9M9A",
        "colab_type": "text"
      },
      "source": [
        "## Memory Architecutre\n",
        "### GPU의 메모리 구조를 고려한 최적 Coding\n",
        "\n",
        "- Local Memory\n",
        "- Global Memory\n",
        "- Shared Memory\n",
        "\n",
        "### 참조\n",
        "\n",
        "- https://github.com/jeonggunlee/cs344"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zIW-1c583Rl",
        "colab_type": "code",
        "outputId": "30667857-6f36-4606-dc11-409ea3c72ccd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile memoryType.cu\n",
        "\n",
        "// Convenience function for checking CUDA runtime API results\n",
        "// can be wrapped around any runtime API call. No-op in release builds.\n",
        "inline\n",
        "cudaError_t checkCuda(cudaError_t result)\n",
        "{\n",
        "#if defined(DEBUG) || defined(_DEBUG)\n",
        "  if (result != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Runtime Error: %s\\n\", cudaGetErrorString(result));\n",
        "    assert(result == cudaSuccess);\n",
        "  }\n",
        "#endif\n",
        "  return result;\n",
        "}\n",
        "\n",
        "\n",
        "// CUDA에서 제공하는 서로 다른 타입의 메모리 공간 활용하기\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "/**********************\n",
        " * using local memory *\n",
        " **********************/\n",
        "\n",
        "// a __device__ or __global__ function runs on the GPU\n",
        "__global__ void use_local_memory_GPU(float in)\n",
        "{\n",
        "    float f;    // variable \"f\" is in local memory and private to each thread\n",
        "    f = in;     // parameter \"in\" is in local memory and private to each thread\n",
        "    // ... real code would presumably do other stuff here ... \n",
        "\n",
        "    // ADDED\n",
        "    int i, index = threadIdx.x;\n",
        "    float average, sum = 0.0f;\n",
        "\n",
        "    for (i=0; i<index; i++) { sum += i; }\n",
        "    average = sum / (index + 1.0f);\n",
        "\n",
        "    printf(\"%f\\n\", average);\n",
        "    \n",
        "}\n",
        "\n",
        "/**********************\n",
        " * using global memory *\n",
        " **********************/\n",
        "\n",
        "// a __global__ function runs on the GPU & can be called from host\n",
        "__global__ void use_global_memory_GPU(float *array)\n",
        "{\n",
        "    int i, index = threadIdx.x;\n",
        "    float average, sum = 0.0f;\n",
        "\n",
        "    // \"array\" is a pointer into global memory on the device\n",
        "    // array[threadIdx.x] = 2.0f * (float) threadIdx.x;\n",
        "\n",
        "    for (i=0; i<index; i++) { sum += array[i]; }\n",
        "    average = sum / (index + 1.0f);\n",
        "    \n",
        "    printf(\"%f\\n\", average);\n",
        "\n",
        "}\n",
        "\n",
        "/**********************\n",
        " * using shared memory *\n",
        " **********************/\n",
        "\n",
        "// (for clarity, hardcoding 128 threads/elements and omitting out-of-bounds checks)\n",
        "__global__ void use_shared_memory_GPU(float *array)\n",
        "{\n",
        "    // local variables, private to each thread\n",
        "    int i, index = threadIdx.x;\n",
        "    float average, sum = 0.0f;\n",
        "\n",
        "    // __shared__ variables are visible to all threads in the thread block\n",
        "    // and have the same lifetime as the thread block\n",
        "    __shared__ float sh_arr[128];\n",
        "\n",
        "    // copy data from \"array\" in global memory to sh_arr in shared memory.\n",
        "    // here, each thread is responsible for copying a single element.\n",
        "    sh_arr[index] = array[index];\n",
        "\n",
        "    __syncthreads();    // ensure all the writes to shared memory have completed\n",
        "\n",
        "    // now, sh_arr is fully populated. Let's find the average of all previous elements\n",
        "    for (i=0; i<index; i++) { sum += sh_arr[i]; }\n",
        "    average = sum / (index + 1.0f);\n",
        "\n",
        "    printf(\"%f\\n\", average);\n",
        "    if (array[index] > average) { array[index] = average; }\n",
        "\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    int blockSize = 256;\n",
        "    int nBlock= 1024*1024;\n",
        "    float ms;\n",
        "    cudaEvent_t startEvent, stopEvent;\n",
        "    \n",
        "    checkCuda( cudaEventCreate(&startEvent) );\n",
        "    checkCuda( cudaEventCreate(&stopEvent) );\n",
        "    \n",
        "    /*\n",
        "     * First, call a kernel that shows using local memory \n",
        "     */\n",
        "    checkCuda( cudaEventRecord(startEvent,0) );\n",
        "    use_local_memory_GPU<<<nBlock, blockSize>>>(2.0f);\n",
        "    checkCuda( cudaEventRecord(stopEvent,0) );\n",
        "    checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "    checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "    printf(\"%f\\n\", ms);\n",
        "    \n",
        "    /*\n",
        "     * Next, call a kernel that shows using global memory\n",
        "     */\n",
        "    float h_arr[128];   // convention: h_ variables live on host\n",
        "    float *d_arr;       // convention: d_ variables live on device (GPU global mem)\n",
        "\n",
        "    // allocate global memory on the device, place result in \"d_arr\"\n",
        "    cudaMalloc((void **) &d_arr, sizeof(float) * blockSize);\n",
        "    // now copy data from host memory \"h_arr\" to device memory \"d_arr\"\n",
        "    cudaMemcpy((void *)d_arr, (void *)h_arr, sizeof(float) * blockSize, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // launch the kernel (1 block of 128 threads)\n",
        "    checkCuda( cudaEventRecord(startEvent,0) );\n",
        "    use_global_memory_GPU<<<nBlock, blockSize>>>(d_arr);  // modifies the contents of array at d_arr\n",
        "    checkCuda( cudaEventRecord(stopEvent,0) );\n",
        "    checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "    checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "    printf(\"%f\\n\", ms);\n",
        "\n",
        "    \n",
        "    // copy the modified array back to the host, overwriting contents of h_arr\n",
        "    cudaMemcpy((void *)h_arr, (void *)d_arr, sizeof(float) * blockSize, cudaMemcpyDeviceToHost);\n",
        "    // ... do other stuff ...\n",
        "\n",
        "    /*\n",
        "     * Next, call a kernel that shows using shared memory\n",
        "     */\n",
        "\n",
        "    // as before, pass in a pointer to data in global memory\n",
        "    checkCuda( cudaEventRecord(startEvent,0) );\n",
        "    use_shared_memory_GPU<<<nBlock, blockSize>>>(d_arr); \n",
        "    checkCuda( cudaEventRecord(stopEvent,0) );\n",
        "    checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "    checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "    printf(\"%f\\n\", ms);\n",
        "\n",
        "    \n",
        "    // copy the modified array back to the host\n",
        "    cudaMemcpy((void *)h_arr, (void *)d_arr, sizeof(float) * blockSize, cudaMemcpyHostToDevice);\n",
        "    // ... do other stuff ...\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting memoryType.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cuuy1wx9C6q",
        "colab_type": "code",
        "outputId": "3a5522b0-c1ce-48a1-82c4-54609b2d5e6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "!nvcc -o memoryType memoryType.cu"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "memoryType.cu(28): warning: variable \"f\" was set but never used\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY_CmtQ-9FIB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d3cd579c-4ed1-442f-81b6-090d95e89b78"
      },
      "source": [
        "!./memoryType"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.000000\n",
            "0.000000\n",
            "0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTQ2WVPM9HVa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "0985e0d5-3ec0-4bca-dfb7-73302e99abfc"
      },
      "source": [
        "!nvprof ./memoryType"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======== Warning: CUDA device error, GPU profiling skipped\n",
            "0.000000\n",
            "0.000000\n",
            "0.000000\n",
            "======== Warning: No profile data collected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDm9OcUwR7sk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}