{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_memoryType.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonggunlee/CUDATeaching/blob/master/01_cuda_lab/07_memoryType.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB9CAgYq9M9A",
        "colab_type": "text"
      },
      "source": [
        "## Memory Architecutre\n",
        "### GPU의 메모리 구조를 고려한 최적 Coding\n",
        "\n",
        "- Local Memory\n",
        "- Global Memory\n",
        "- Shared Memory\n",
        "\n",
        "### 참조\n",
        "\n",
        "- https://github.com/jeonggunlee/cs344"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zIW-1c583Rl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "966052f6-2a5a-4361-ebad-8ae1a909cdf0"
      },
      "source": [
        "%%writefile memoryType.cu\n",
        "\n",
        "// CUDA에서 제공하는 서로 다른 타입의 메모리 공간 활용하기\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "/**********************\n",
        " * using local memory *\n",
        " **********************/\n",
        "\n",
        "// a __device__ or __global__ function runs on the GPU\n",
        "__global__ void use_local_memory_GPU(float in)\n",
        "{\n",
        "    float f;    // variable \"f\" is in local memory and private to each thread\n",
        "    f = in;     // parameter \"in\" is in local memory and private to each thread\n",
        "    // ... real code would presumably do other stuff here ... \n",
        "}\n",
        "\n",
        "/**********************\n",
        " * using global memory *\n",
        " **********************/\n",
        "\n",
        "// a __global__ function runs on the GPU & can be called from host\n",
        "__global__ void use_global_memory_GPU(float *array)\n",
        "{\n",
        "    // \"array\" is a pointer into global memory on the device\n",
        "    array[threadIdx.x] = 2.0f * (float) threadIdx.x;\n",
        "}\n",
        "\n",
        "/**********************\n",
        " * using shared memory *\n",
        " **********************/\n",
        "\n",
        "// (for clarity, hardcoding 128 threads/elements and omitting out-of-bounds checks)\n",
        "__global__ void use_shared_memory_GPU(float *array)\n",
        "{\n",
        "    // local variables, private to each thread\n",
        "    int i, index = threadIdx.x;\n",
        "    float average, sum = 0.0f;\n",
        "\n",
        "    // __shared__ variables are visible to all threads in the thread block\n",
        "    // and have the same lifetime as the thread block\n",
        "    __shared__ float sh_arr[128];\n",
        "\n",
        "    // copy data from \"array\" in global memory to sh_arr in shared memory.\n",
        "    // here, each thread is responsible for copying a single element.\n",
        "    sh_arr[index] = array[index];\n",
        "\n",
        "    __syncthreads();    // ensure all the writes to shared memory have completed\n",
        "\n",
        "    // now, sh_arr is fully populated. Let's find the average of all previous elements\n",
        "    for (i=0; i<index; i++) { sum += sh_arr[i]; }\n",
        "    average = sum / (index + 1.0f);\n",
        "\n",
        "    // if array[index] is greater than the average of array[0..index-1], replace with average.\n",
        "    // since array[] is in global memory, this change will be seen by the host (and potentially \n",
        "    // other thread blocks, if any)\n",
        "    if (array[index] > average) { array[index] = average; }\n",
        "\n",
        "    // the following code has NO EFFECT: it modifies shared memory, but \n",
        "    // the resulting modified data is never copied back to global memory\n",
        "    // and vanishes when the thread block completes\n",
        "    sh_arr[index] = 3.14;\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    /*\n",
        "     * First, call a kernel that shows using local memory \n",
        "     */\n",
        "    use_local_memory_GPU<<<1, 128>>>(2.0f);\n",
        "\n",
        "    /*\n",
        "     * Next, call a kernel that shows using global memory\n",
        "     */\n",
        "    float h_arr[128];   // convention: h_ variables live on host\n",
        "    float *d_arr;       // convention: d_ variables live on device (GPU global mem)\n",
        "\n",
        "    // allocate global memory on the device, place result in \"d_arr\"\n",
        "    cudaMalloc((void **) &d_arr, sizeof(float) * 128);\n",
        "    // now copy data from host memory \"h_arr\" to device memory \"d_arr\"\n",
        "    cudaMemcpy((void *)d_arr, (void *)h_arr, sizeof(float) * 128, cudaMemcpyHostToDevice);\n",
        "    // launch the kernel (1 block of 128 threads)\n",
        "    use_global_memory_GPU<<<1, 128>>>(d_arr);  // modifies the contents of array at d_arr\n",
        "    // copy the modified array back to the host, overwriting contents of h_arr\n",
        "    cudaMemcpy((void *)h_arr, (void *)d_arr, sizeof(float) * 128, cudaMemcpyDeviceToHost);\n",
        "    // ... do other stuff ...\n",
        "\n",
        "    /*\n",
        "     * Next, call a kernel that shows using shared memory\n",
        "     */\n",
        "\n",
        "    // as before, pass in a pointer to data in global memory\n",
        "    use_shared_memory_GPU<<<1, 128>>>(d_arr); \n",
        "    // copy the modified array back to the host\n",
        "    cudaMemcpy((void *)h_arr, (void *)d_arr, sizeof(float) * 128, cudaMemcpyHostToDevice);\n",
        "    // ... do other stuff ...\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing memoryType.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cuuy1wx9C6q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "598c6789-02cc-4a41-a4d7-a3a4e67c9a4e"
      },
      "source": [
        "!nvcc -o memoryType memoryType.cu"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "memoryType.cu(12): warning: variable \"f\" was set but never used\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY_CmtQ-9FIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!./memoryType"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTQ2WVPM9HVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}