{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_Transpose.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonggunlee/CUDATeaching/blob/master/02_cuda_lab/01_Transpose.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UT_qN20klON",
        "colab_type": "text"
      },
      "source": [
        "## Matrix Transpose Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJLKazMijoQF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "53fb8254-5dd7-43a9-91b7-fd8310a17951"
      },
      "source": [
        "%%writefile copy.cu\n",
        "\n",
        "/* Copyright (c) 1993-2015, NVIDIA CORPORATION. All rights reserved.\n",
        " *\n",
        " * Redistribution and use in source and binary forms, with or without\n",
        " * modification, are permitted provided that the following conditions\n",
        " * are met:\n",
        " *  * Redistributions of source code must retain the above copyright\n",
        " *    notice, this list of conditions and the following disclaimer.\n",
        " *  * Redistributions in binary form must reproduce the above copyright\n",
        " *    notice, this list of conditions and the following disclaimer in the\n",
        " *    documentation and/or other materials provided with the distribution.\n",
        " *  * Neither the name of NVIDIA CORPORATION nor the names of its\n",
        " *    contributors may be used to endorse or promote products derived\n",
        " *    from this software without specific prior written permission.\n",
        " *\n",
        " * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n",
        " * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
        " * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n",
        " * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n",
        " * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n",
        " * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n",
        " * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n",
        " * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n",
        " * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
        " * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        " * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        " */\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "// Convenience function for checking CUDA runtime API results\n",
        "// can be wrapped around any runtime API call. No-op in release builds.\n",
        "inline\n",
        "cudaError_t checkCuda(cudaError_t result)\n",
        "{\n",
        "#if defined(DEBUG) || defined(_DEBUG)\n",
        "  if (result != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Runtime Error: %s\\n\", cudaGetErrorString(result));\n",
        "    assert(result == cudaSuccess);\n",
        "  }\n",
        "#endif\n",
        "  return result;\n",
        "}\n",
        "\n",
        "const int TILE_DIM = 32;\n",
        "const int BLOCK_ROWS = 8;\n",
        "const int NUM_REPS = 100;\n",
        "\n",
        "// Check errors and print GB/s\n",
        "void postprocess(const float *ref, const float *res, int n, float ms)\n",
        "{\n",
        "  bool passed = true;\n",
        "  for (int i = 0; i < n; i++)\n",
        "    if (res[i] != ref[i]) {\n",
        "      printf(\"%d %f %f\\n\", i, res[i], ref[i]);\n",
        "      printf(\"%25s\\n\", \"*** FAILED ***\");\n",
        "      passed = false;\n",
        "      break;\n",
        "    }\n",
        "  if (passed)\n",
        "    printf(\"%20.2f\\n\", 2 * n * sizeof(float) * 1e-6 * NUM_REPS / ms );\n",
        "}\n",
        "\n",
        "// simple copy kernel\n",
        "// Used as reference case representing best effective bandwidth.\n",
        "__global__ void copy(float *odata, const float *idata)\n",
        "{\n",
        "  int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
        "  int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
        "  int width = gridDim.x * TILE_DIM;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j+= BLOCK_ROWS)\n",
        "    odata[(y+j)*width + x] = idata[(y+j)*width + x];\n",
        "}\n",
        "\n",
        "// copy kernel using shared memory\n",
        "// Also used as reference case, demonstrating effect of using shared memory.\n",
        "__global__ void copySharedMem(float *odata, const float *idata)\n",
        "{\n",
        "  __shared__ float tile[TILE_DIM * TILE_DIM];\n",
        "  \n",
        "  int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
        "  int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
        "  int width = gridDim.x * TILE_DIM;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)\n",
        "     tile[(threadIdx.y+j)*TILE_DIM + threadIdx.x] = idata[(y+j)*width + x];\n",
        "\n",
        "  __syncthreads();\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)\n",
        "     odata[(y+j)*width + x] = tile[(threadIdx.y+j)*TILE_DIM + threadIdx.x];          \n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "  const int nx = 1024;\n",
        "  const int ny = 1024;\n",
        "  const int mem_size = nx*ny*sizeof(float);\n",
        "\n",
        "  dim3 dimGrid(nx/TILE_DIM, ny/TILE_DIM, 1);\n",
        "  dim3 dimBlock(TILE_DIM, BLOCK_ROWS, 1);\n",
        "\n",
        "  int devId = 0;\n",
        "  if (argc > 1) devId = atoi(argv[1]);\n",
        "\n",
        "  cudaDeviceProp prop;\n",
        "  checkCuda( cudaGetDeviceProperties(&prop, devId));\n",
        "  printf(\"\\nDevice : %s\\n\", prop.name);\n",
        "  printf(\"Matrix size: %d %d, Block size: %d %d, Tile size: %d %d\\n\", \n",
        "         nx, ny, TILE_DIM, BLOCK_ROWS, TILE_DIM, TILE_DIM);\n",
        "  printf(\"dimGrid: %d %d %d. dimBlock: %d %d %d\\n\",\n",
        "         dimGrid.x, dimGrid.y, dimGrid.z, dimBlock.x, dimBlock.y, dimBlock.z);\n",
        "  \n",
        "  checkCuda( cudaSetDevice(devId) );\n",
        "\n",
        "  float *h_idata = (float*)malloc(mem_size);\n",
        "  float *h_cdata = (float*)malloc(mem_size);\n",
        "  float *h_tdata = (float*)malloc(mem_size);\n",
        "  float *gold    = (float*)malloc(mem_size);\n",
        "  \n",
        "  float *d_idata, *d_cdata, *d_tdata;\n",
        "  checkCuda( cudaMalloc(&d_idata, mem_size) );\n",
        "  checkCuda( cudaMalloc(&d_cdata, mem_size) );\n",
        "  checkCuda( cudaMalloc(&d_tdata, mem_size) );\n",
        "\n",
        "  // check parameters and calculate execution configuration\n",
        "  if (nx % TILE_DIM || ny % TILE_DIM) {\n",
        "    printf(\"nx and ny must be a multiple of TILE_DIM\\n\");\n",
        "    goto error_exit;\n",
        "  }\n",
        "\n",
        "  if (TILE_DIM % BLOCK_ROWS) {\n",
        "    printf(\"TILE_DIM must be a multiple of BLOCK_ROWS\\n\");\n",
        "    goto error_exit;\n",
        "  }\n",
        "    \n",
        "  // host\n",
        "  for (int j = 0; j < ny; j++)\n",
        "    for (int i = 0; i < nx; i++)\n",
        "      h_idata[j*nx + i] = j*nx + i;\n",
        "\n",
        "  // correct result for error checking\n",
        "  for (int j = 0; j < ny; j++)\n",
        "    for (int i = 0; i < nx; i++)\n",
        "      gold[j*nx + i] = h_idata[i*nx + j];\n",
        "  \n",
        "  // device\n",
        "  checkCuda( cudaMemcpy(d_idata, h_idata, mem_size, cudaMemcpyHostToDevice) );\n",
        "  \n",
        "  // events for timing\n",
        "  cudaEvent_t startEvent, stopEvent;\n",
        "  checkCuda( cudaEventCreate(&startEvent) );\n",
        "  checkCuda( cudaEventCreate(&stopEvent) );\n",
        "  float ms;\n",
        "\n",
        "  // ------------\n",
        "  // time kernels\n",
        "  // ------------\n",
        "  printf(\"%25s%25s\\n\", \"Routine\", \"Bandwidth (GB/s)\");\n",
        "  \n",
        "  // ----\n",
        "  // copy \n",
        "  // ----\n",
        "  printf(\"%25s\", \"copy\");\n",
        "  checkCuda( cudaMemset(d_cdata, 0, mem_size) );\n",
        "  // warm up\n",
        "  copy<<<dimGrid, dimBlock>>>(d_cdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(startEvent, 0) );\n",
        "  for (int i = 0; i < NUM_REPS; i++)\n",
        "     copy<<<dimGrid, dimBlock>>>(d_cdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(stopEvent, 0) );\n",
        "  checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "  checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "  checkCuda( cudaMemcpy(h_cdata, d_cdata, mem_size, cudaMemcpyDeviceToHost) );\n",
        "  postprocess(h_idata, h_cdata, nx*ny, ms);\n",
        "\n",
        "  // -------------\n",
        "  // copySharedMem \n",
        "  // -------------\n",
        "  printf(\"%25s\", \"shared memory copy\");\n",
        "  checkCuda( cudaMemset(d_cdata, 0, mem_size) );\n",
        "  // warm up\n",
        "  copySharedMem<<<dimGrid, dimBlock>>>(d_cdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(startEvent, 0) );\n",
        "  for (int i = 0; i < NUM_REPS; i++)\n",
        "     copySharedMem<<<dimGrid, dimBlock>>>(d_cdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(stopEvent, 0) );\n",
        "  checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "  checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "  checkCuda( cudaMemcpy(h_cdata, d_cdata, mem_size, cudaMemcpyDeviceToHost) );\n",
        "  postprocess(h_idata, h_cdata, nx * ny, ms);\n",
        "\n",
        "\n",
        "error_exit:\n",
        "  // cleanup\n",
        "  checkCuda( cudaEventDestroy(startEvent) );\n",
        "  checkCuda( cudaEventDestroy(stopEvent) );\n",
        "  checkCuda( cudaFree(d_tdata) );\n",
        "  checkCuda( cudaFree(d_cdata) );\n",
        "  checkCuda( cudaFree(d_idata) );\n",
        "  free(h_idata);\n",
        "  free(h_tdata);\n",
        "  free(h_cdata);\n",
        "  free(gold);\n",
        "}"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing copy.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7E6xCy9kEM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc -o copy copy.cu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7STxyl7kKRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "f83b927c-d6c6-47e0-bc14-5955d2a7958c"
      },
      "source": [
        "!./copy"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Device : Tesla T4\n",
            "Matrix size: 1024 1024, Block size: 32 8, Tile size: 32 32\n",
            "dimGrid: 32 32 1. dimBlock: 32 8 1\n",
            "                  Routine         Bandwidth (GB/s)\n",
            "                     copy              212.45\n",
            "       shared memory copy              229.76\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5I_sbPLh_kn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5aeb7bc8-9bb7-4613-f0b0-06168247ddb5"
      },
      "source": [
        "%%writefile transpose.cu\n",
        "\n",
        "/* Copyright (c) 1993-2015, NVIDIA CORPORATION. All rights reserved.\n",
        " *\n",
        " * Redistribution and use in source and binary forms, with or without\n",
        " * modification, are permitted provided that the following conditions\n",
        " * are met:\n",
        " *  * Redistributions of source code must retain the above copyright\n",
        " *    notice, this list of conditions and the following disclaimer.\n",
        " *  * Redistributions in binary form must reproduce the above copyright\n",
        " *    notice, this list of conditions and the following disclaimer in the\n",
        " *    documentation and/or other materials provided with the distribution.\n",
        " *  * Neither the name of NVIDIA CORPORATION nor the names of its\n",
        " *    contributors may be used to endorse or promote products derived\n",
        " *    from this software without specific prior written permission.\n",
        " *\n",
        " * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n",
        " * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
        " * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n",
        " * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n",
        " * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n",
        " * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n",
        " * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n",
        " * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n",
        " * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
        " * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        " * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        " */\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "// Convenience function for checking CUDA runtime API results\n",
        "// can be wrapped around any runtime API call. No-op in release builds.\n",
        "inline\n",
        "cudaError_t checkCuda(cudaError_t result)\n",
        "{\n",
        "#if defined(DEBUG) || defined(_DEBUG)\n",
        "  if (result != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Runtime Error: %s\\n\", cudaGetErrorString(result));\n",
        "    assert(result == cudaSuccess);\n",
        "  }\n",
        "#endif\n",
        "  return result;\n",
        "}\n",
        "\n",
        "const int TILE_DIM = 32;\n",
        "const int BLOCK_ROWS = 8;\n",
        "const int NUM_REPS = 100;\n",
        "\n",
        "// Check errors and print GB/s\n",
        "void postprocess(const float *ref, const float *res, int n, float ms)\n",
        "{\n",
        "  bool passed = true;\n",
        "  for (int i = 0; i < n; i++)\n",
        "    if (res[i] != ref[i]) {\n",
        "      printf(\"%d %f %f\\n\", i, res[i], ref[i]);\n",
        "      printf(\"%25s\\n\", \"*** FAILED ***\");\n",
        "      passed = false;\n",
        "      break;\n",
        "    }\n",
        "  if (passed)\n",
        "    printf(\"%20.2f\\n\", 2 * n * sizeof(float) * 1e-6 * NUM_REPS / ms );\n",
        "}\n",
        "\n",
        "// naive transpose\n",
        "// Simplest transpose; doesn't use shared memory.\n",
        "// Global memory reads are coalesced but writes are not.\n",
        "__global__ void transposeNaive(float *odata, const float *idata)\n",
        "{\n",
        "  int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
        "  int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
        "  int width = gridDim.x * TILE_DIM;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j+= BLOCK_ROWS)\n",
        "    odata[x*width + (y+j)] = idata[(y+j)*width + x];\n",
        "}\n",
        "\n",
        "// coalesced transpose\n",
        "// Uses shared memory to achieve coalesing in both reads and writes\n",
        "// Tile width == #banks causes shared memory bank conflicts.\n",
        "__global__ void transposeCoalesced(float *odata, const float *idata)\n",
        "{\n",
        "  __shared__ float tile[TILE_DIM][TILE_DIM];\n",
        "    \n",
        "  int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
        "  int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
        "  int width = gridDim.x * TILE_DIM;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)\n",
        "     tile[threadIdx.y+j][threadIdx.x] = idata[(y+j)*width + x];\n",
        "\n",
        "  __syncthreads();\n",
        "\n",
        "  x = blockIdx.y * TILE_DIM + threadIdx.x;  // transpose block offset\n",
        "  y = blockIdx.x * TILE_DIM + threadIdx.y;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)\n",
        "     odata[(y+j)*width + x] = tile[threadIdx.x][threadIdx.y + j];\n",
        "}\n",
        "   \n",
        "\n",
        "// No bank-conflict transpose\n",
        "// Same as transposeCoalesced except the first tile dimension is padded \n",
        "// to avoid shared memory bank conflicts.\n",
        "__global__ void transposeNoBankConflicts(float *odata, const float *idata)\n",
        "{\n",
        "  __shared__ float tile[TILE_DIM][TILE_DIM+1];\n",
        "    \n",
        "  int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
        "  int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
        "  int width = gridDim.x * TILE_DIM;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)\n",
        "     tile[threadIdx.y+j][threadIdx.x] = idata[(y+j)*width + x];\n",
        "\n",
        "  __syncthreads();\n",
        "\n",
        "  x = blockIdx.y * TILE_DIM + threadIdx.x;  // transpose block offset\n",
        "  y = blockIdx.x * TILE_DIM + threadIdx.y;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)\n",
        "     odata[(y+j)*width + x] = tile[threadIdx.x][threadIdx.y + j];\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "  const int nx = 1024;\n",
        "  const int ny = 1024;\n",
        "  const int mem_size = nx*ny*sizeof(float);\n",
        "\n",
        "  dim3 dimGrid(nx/TILE_DIM, ny/TILE_DIM, 1);\n",
        "  dim3 dimBlock(TILE_DIM, BLOCK_ROWS, 1);\n",
        "\n",
        "  int devId = 0;\n",
        "  if (argc > 1) devId = atoi(argv[1]);\n",
        "\n",
        "  cudaDeviceProp prop;\n",
        "  checkCuda( cudaGetDeviceProperties(&prop, devId));\n",
        "  printf(\"\\nDevice : %s\\n\", prop.name);\n",
        "  printf(\"Matrix size: %d %d, Block size: %d %d, Tile size: %d %d\\n\", \n",
        "         nx, ny, TILE_DIM, BLOCK_ROWS, TILE_DIM, TILE_DIM);\n",
        "  printf(\"dimGrid: %d %d %d. dimBlock: %d %d %d\\n\",\n",
        "         dimGrid.x, dimGrid.y, dimGrid.z, dimBlock.x, dimBlock.y, dimBlock.z);\n",
        "  \n",
        "  checkCuda( cudaSetDevice(devId) );\n",
        "\n",
        "  float *h_idata = (float*)malloc(mem_size);\n",
        "  float *h_cdata = (float*)malloc(mem_size);\n",
        "  float *h_tdata = (float*)malloc(mem_size);\n",
        "  float *gold    = (float*)malloc(mem_size);\n",
        "  \n",
        "  float *d_idata, *d_cdata, *d_tdata;\n",
        "  checkCuda( cudaMalloc(&d_idata, mem_size) );\n",
        "  checkCuda( cudaMalloc(&d_cdata, mem_size) );\n",
        "  checkCuda( cudaMalloc(&d_tdata, mem_size) );\n",
        "\n",
        "  // check parameters and calculate execution configuration\n",
        "  if (nx % TILE_DIM || ny % TILE_DIM) {\n",
        "    printf(\"nx and ny must be a multiple of TILE_DIM\\n\");\n",
        "    goto error_exit;\n",
        "  }\n",
        "\n",
        "  if (TILE_DIM % BLOCK_ROWS) {\n",
        "    printf(\"TILE_DIM must be a multiple of BLOCK_ROWS\\n\");\n",
        "    goto error_exit;\n",
        "  }\n",
        "    \n",
        "  // host\n",
        "  for (int j = 0; j < ny; j++)\n",
        "    for (int i = 0; i < nx; i++)\n",
        "      h_idata[j*nx + i] = j*nx + i;\n",
        "\n",
        "  // correct result for error checking\n",
        "  for (int j = 0; j < ny; j++)\n",
        "    for (int i = 0; i < nx; i++)\n",
        "      gold[j*nx + i] = h_idata[i*nx + j];\n",
        "  \n",
        "  // device\n",
        "  checkCuda( cudaMemcpy(d_idata, h_idata, mem_size, cudaMemcpyHostToDevice) );\n",
        "  \n",
        "  // events for timing\n",
        "  cudaEvent_t startEvent, stopEvent;\n",
        "  checkCuda( cudaEventCreate(&startEvent) );\n",
        "  checkCuda( cudaEventCreate(&stopEvent) );\n",
        "  float ms;\n",
        "\n",
        "  // ------------\n",
        "  // time kernels\n",
        "  // ------------\n",
        "  printf(\"%25s%25s\\n\", \"Routine\", \"Bandwidth (GB/s)\");\n",
        "\n",
        "  // --------------\n",
        "  // transposeNaive \n",
        "  // --------------\n",
        "  printf(\"%25s\", \"naive transpose\");\n",
        "  checkCuda( cudaMemset(d_tdata, 0, mem_size) );\n",
        "  // warmup\n",
        "  transposeNaive<<<dimGrid, dimBlock>>>(d_tdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(startEvent, 0) );\n",
        "  for (int i = 0; i < NUM_REPS; i++)\n",
        "     transposeNaive<<<dimGrid, dimBlock>>>(d_tdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(stopEvent, 0) );\n",
        "  checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "  checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "  checkCuda( cudaMemcpy(h_tdata, d_tdata, mem_size, cudaMemcpyDeviceToHost) );\n",
        "  postprocess(gold, h_tdata, nx * ny, ms);\n",
        "\n",
        "  // ------------------\n",
        "  // transposeCoalesced \n",
        "  // ------------------\n",
        "  printf(\"%25s\", \"coalesced transpose\");\n",
        "  checkCuda( cudaMemset(d_tdata, 0, mem_size) );\n",
        "  // warmup\n",
        "  transposeCoalesced<<<dimGrid, dimBlock>>>(d_tdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(startEvent, 0) );\n",
        "  for (int i = 0; i < NUM_REPS; i++)\n",
        "     transposeCoalesced<<<dimGrid, dimBlock>>>(d_tdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(stopEvent, 0) );\n",
        "  checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "  checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "  checkCuda( cudaMemcpy(h_tdata, d_tdata, mem_size, cudaMemcpyDeviceToHost) );\n",
        "  postprocess(gold, h_tdata, nx * ny, ms);\n",
        "\n",
        "  // ------------------------\n",
        "  // transposeNoBankConflicts\n",
        "  // ------------------------\n",
        "  printf(\"%25s\", \"conflict-free transpose\");\n",
        "  checkCuda( cudaMemset(d_tdata, 0, mem_size) );\n",
        "  // warmup\n",
        "  transposeNoBankConflicts<<<dimGrid, dimBlock>>>(d_tdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(startEvent, 0) );\n",
        "  for (int i = 0; i < NUM_REPS; i++)\n",
        "     transposeNoBankConflicts<<<dimGrid, dimBlock>>>(d_tdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(stopEvent, 0) );\n",
        "  checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "  checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "  checkCuda( cudaMemcpy(h_tdata, d_tdata, mem_size, cudaMemcpyDeviceToHost) );\n",
        "  postprocess(gold, h_tdata, nx * ny, ms);\n",
        "\n",
        "error_exit:\n",
        "  // cleanup\n",
        "  checkCuda( cudaEventDestroy(startEvent) );\n",
        "  checkCuda( cudaEventDestroy(stopEvent) );\n",
        "  checkCuda( cudaFree(d_tdata) );\n",
        "  checkCuda( cudaFree(d_cdata) );\n",
        "  checkCuda( cudaFree(d_idata) );\n",
        "  free(h_idata);\n",
        "  free(h_tdata);\n",
        "  free(h_cdata);\n",
        "  free(gold);\n",
        "}"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting transpose.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4qZUeGJiyX5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e055e10a-698d-4a12-d722-d4b1b0375afc"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "copy  copy.cu  sample_data  transpose  transpose.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPqutafhi1Q4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc -o transpose transpose.cu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MX15rCYjKg3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "98484f08-3bc9-4fed-b358-79ddcca9ae46"
      },
      "source": [
        "!./transpose"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Device : Tesla T4\n",
            "Matrix size: 1024 1024, Block size: 32 8, Tile size: 32 32\n",
            "dimGrid: 32 32 1. dimBlock: 32 8 1\n",
            "                  Routine         Bandwidth (GB/s)\n",
            "          naive transpose               44.46\n",
            "      coalesced transpose               74.86\n",
            "  conflict-free transpose              203.88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ8YLJJTjM3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}